# 대기열 구현
티켓팅 서비스의 안정성과 순서 보장이 필요한 요구사항을 반영하기 위해 대기열을 구현

## 요구사항
* 대기열이 다른 서비스에 영향을 미치지 않아야 한다.
* 유저는 들어온 순서대로 티켓 구매 서비스를 제공받아야 한다.
* 다량의 유저가 대기열에 진입할 수 있어야 한다.

### 구조 및 로직
* Waiting queue : 대기중인 유저를 담고 있는 큐
* Entry queue : waiting queue에서 빠져나와 결제를 진행중인 유저를 담고 있는 큐
* Dispatch stream : 대기중인 유저가 빠져나왔음을 전달하기 위한 메시지를 담고 있는 stream

#### 대기열 진입
1. 클라이언트는 Broker server로 대기열 진입을 요청합니다.
2. Broker는 Waiting queue에 유저 정보와 번호(index)를 할당해 저장합니다. (ZADD WAITING:{eventId} idx user-data)
3. 이후 Broker는 1초마다 클라이언트에게 SSE를 통해 순번 정보를 전송합니다. ( ZRANK WAITING:{eventId} )

#### 대기열 탈출
Dispatcher server는 1초마다 다음을 진행합니다.
1. WAITING:*으로 대기열 탈출이 필요한 모든 큐를 조회합니다.
2. 각 큐마다 정해진 숫자만큼 유저를 대기열로부터 탈출시킵니다.
3. 대기열 탈출 이후, 무작위 토큰 값을 생성합니다.
4. 토큰 값과 함께 Dispatch stream으로 탈출 메시지를 적재합니다.

#### 탈출 이후
1. Broker server는 같은 consumer group으로 Dispatch stream의 데이터를 consume합니다. 이때 sse로 연결된 유저의 데이터만 ack합니다.
2. ack한 데이터를 유저에게 전송하고, 유저는 데이터 안의 토큰 값으로 좌석 선택 및 결제를 진행합니다.

### 기술적 고민
#### broker의 scale out
대량의 유저가 sse연결을 유지하고 지속적으로 대기열 순번을 전송받으므로, 많은 부하가 일어난다.
이를 해결하기 위해서 broker server를 확장 가능하도록 설계하는 것이 중요하다.

이때 여러 대의 broker server가 단일 Dispatch Stream으로부터 탈출 정보를 가져오게 되는데,

broker가 연결된 유저의 메시지만 골라서 가져올 방법이 필요하다.

이러한 구조에서 여러개의 broker가 하나의 consumer group을 가지고, 자신이 연결된 유저의 메시지인 경우만 ack하는 방식으로 문제를 해결할 수 있다.

#### Dispatcher의 대기열 탈출 로직의 문제
대기열에 진입한 유저가 많을 수록, 대기열 탈출 로직에 더 많은 시간이 소모된다.
만약 스케쥴링된 시간 이내에 이 작업이 완료되지 못할 경우, 스케쥴링 워커 스레드의 스레드 풀이 고갈될 때까지 작업이 쌓이게 될 것이다.

이 문제를 해결하기 위해서 2가지 방법을 고민해 볼 수 있다.
1. 스케쥴링 된 시간 이내에 작업이 모두 끝나도록 하기
2. 백프레셔 전략 도입

##### 1. 대기열 탈출 로직 개선
대기열 탈출 로직은 Redis를 조작하는 I/O 비율이 높은 작업이다. 그렇기 때문에, 멀티스레딩을 이용해 이를 처리한다면 성능적 이득을 볼 수 있다.

그러나 기존 탈출 로직을 그대로 호출하는 것은 문제가 있다. 기존 로직은 모든 keys WAITING:*로 대기열 키를 가져와, 이 키의 ZSET에서 특정 갯수만큼
유저를 빼 Entry queue로 이동시킨다. 멀티스레딩의 특성상 동시에 같은 키의 ZSET을 호출하게 된다면 치명적인 오류가 발생한다.

이를 다음과 같이 해결했다.
1. 조회한 key를 임시 Redis 저장공간에 LPUSH한다.
2. 새로운 스레드를 생성한다.
3. 스레드는 임시 저장공간에서 RPOP해 작업할 key를 가져온다.
4. key로 대기열 탈출 로직을 실행한다.

##### 2. 백프레셔 전략 도입
그럼에도 불구하고 작업이 쌓여 설정한 스레드 풀이 꽉 차는 경우가 생길 수 있다. 이를 해결하기 위해서 `CallerRunsPolicy`를 도입하였다.

`CallerRunsPolicy`는 스레드풀과 스레드 내부 큐까지 모두 가득차면 스레드를 생성한 스레드(caller thread)에서 이 작업을 실행하도록 하는 것이다.
작업이 완료될 동안 caller thread는 blocking되므로, 새로운 작업을 생성하지 않아 자연스럽게 병목이 해결될 수 있다.

#### 인증/인가
broker를 scale out하고 개별 서버로 분리한 이후, 인증된 유저만 대기열에 진입해야 하는 요구사항을 만족해야 했다.

동일한 토큰 파싱/디코드 코드를 동일한 서버에 두는 것은 유지보수에 불편함을 주었다.
이를 개선하고자 모든 트래픽을 라우팅하는 gateway 서버를 도입했다. gateway서버를 spring cloud gateway를 이용해 구현하고, gateway가 
요청의 헤더, 쿠키 토큰의 서명을 검증하고 페이로드를 헤더에 추가해 라우팅하였다.

이후 라우팅된 서버에서 비즈니스적 인가를 거쳐 API를 호출하였다.

#### 서비스 디스커버리를 위한 eureka server 도입
broker가 scale out 되면서, gateway는 어느 broker로 요청을 라우팅해야 할지 알 수 없게 되었다. 이를 해결하기 위해 클라이언트 로드밸런서인 
유레카 서버를 도입하였다. 유레카 서버 도입 이후 aws에 배포하는데 애를 먹었는데, broker서버의 private ip를 알아내서 hostname으로 설정해 문제 해결

### Redis 영속화 및 비정상 종료 처리
#### Redis 영속화 설정
현재 Redis는 다음과 같이 설정되어 데이터 영속화를 보장합니다:

```yaml
redis:
  image: redis:alpine
  command: redis-server --save 60 1 --loglevel warning
  volumes:
    - ./redis/data:/data
```

- **RDB 스냅샷**: 60초마다 최소 1개의 키가 변경되면 자동 저장
- **볼륨 마운트**: `/data` 디렉토리를 호스트에 영구 보존
- **데이터 보존**: 스트림 메시지, Consumer Group 정보, Pending 메시지 모두 영속화

#### 비정상 종료 시 메시지 재처리 로직
Redis가 비정상 종료되거나 애플리케이션이 재시작될 때 ACK되지 않은 메시지들을 자동으로 재처리하는 로직을 구현했습니다.

##### 1. Broker 모듈 (EntryStreamMessageListener)
```java
@PostConstruct
public void startListening() {
    // 스트림 리스너 시작
    streamMessageListenerContainer.start();
    
    // Redis 재시작 시 pending 메시지들을 재처리
    processPendingMessages(groupName, consumerName);
}

private void processPendingMessages(String groupName, String consumerName) {
    // 현재 컨슈머의 pending 메시지들을 조회
    List<PendingMessage> pendingMessages = redisTemplate.opsForStream()
        .pending(DISPATCH_QUEUE_CHANNEL_NAME, Consumer.from(groupName, consumerName))
        .getMessages();

    // 각 pending 메시지를 재처리
    for (PendingMessage pendingMessage : pendingMessages) {
        // XRANGE로 메시지 데이터를 읽어와서 onMessage()로 재처리
        // 실패한 메시지는 ACK하여 무한 루프 방지
    }
}
```

##### 2. Dispatcher 모듈 (EntryQueueConsumer)
```java
@PostConstruct
public void startListening() {
    // 스트림 컨슈머 시작
    container.receive(Consumer.from(GROUP_NAME, CONSUMER_NAME), ...);
    
    // Redis 재시작 시 pending 메시지들을 재처리
    processPendingMessages();
}
```

##### 3. 재처리 동작 과정
1. **애플리케이션 시작 시**: `@PostConstruct`에서 자동으로 pending 메시지 확인
2. **Pending 메시지 조회**: Redis의 `XPENDING` 명령으로 ACK되지 않은 메시지 목록 조회
3. **메시지 데이터 복구**: `XRANGE` 명령으로 pending 메시지의 실제 데이터 읽기
4. **재처리 실행**: 기존 메시지 처리 로직(`onMessage`, `handleMessage`)으로 재처리
5. **실패 처리**: 재처리 실패 시 해당 메시지를 ACK하여 무한 루프 방지

##### 4. 장애 복구 시나리오
- **Redis 정상 재시작**: 모든 스트림 데이터와 Consumer Group 상태 완전 복구
- **Redis 비정상 종료**: 마지막 스냅샷(최대 60초 전)까지의 데이터 복구
- **애플리케이션 재시작**: 종료 전 처리 중이던 메시지들이 pending 상태로 남아있어 자동 재처리

이를 통해 시스템 장애 상황에서도 대기열 메시지 손실 없이 안정적인 서비스 제공이 가능합니다.

#### 추가로 생각해볼 개선점
현재 blocking 기반의 tomcat으로 broker 서버가 동작하는데, 이걸 Netty 기반의 nonblocking server로 교체하면 성능 개선이 이루어질 것이다.

